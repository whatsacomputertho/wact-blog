
<p>With &#8203;our &#8203;model &#8203;trained and application written, our final task will be to deploy our application.&#160; As with the rest of the project, I wanted to approach this phase of the deployment with both simplicity and long-term scalability in mind.&#160; The deployment process should be semi-automated and repeatable both locally and on remote machines.&#160; It should be possible to deploy the FBSim application on a generic linux VM, and on a Kubernetes cluster.</p>
<p>As such, the obvious choice would be to containerize the FBSim application, and to develop both a docker-compose-based deployment, and a Kubernetes-based deployment.&#160; To support these deployments and satisfy the "semi-automated" requirement, I would need to develop CI to regularly build my container images.<br></p>
<h2 data-original-level="H2" id="header-694c2fe0-5c44-9ccd-289a-c76a9f6452f7">Continuous &#8203;Integration</h2>
<p>I developed CI applied to both the fbsim-ui and fbsim-api repositories.&#160; Across both repositories, the CI followed a very similar pattern.</p>
<ol><li role="textbox">Write a Containerfile that builds ​the ​application ​and/or copies the application into the image<br></li><li role="textbox">Write a GitHub actions workflow that runs the containerized build and pushes the built image to ghcr.io</li><li role="textbox">Use the repository's package manifest (cargo.toml, package.json) to version &amp; tag the images in ghcr.io<br></li></ol>
<h3 data-original-level="H3" id="header-afd4f595-af5a-075b-ff0e-499c87048aed">Containerization<br></h3>
<p>For the fbsim-ui, the containerization process was quite simple.&#160; Since JavaScript is an interpreted language, I would just need to make sure that I use a base image with NodeJS built in, then copy my source code and static content directly into the image in its expected directory structure.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content"># Copy files into builder image for single-layer copy into main image
FROM scratch AS copyfiles
COPY src/*.js /copyfiles/src/
COPY static/css/*.css /copyfiles/static/css/
COPY static/img/*.png /copyfiles/static/img/
COPY static/img/*.jpg /copyfiles/static/img/
COPY static/js/*.js /copyfiles/static/js/
COPY static/views/*.hbs /copyfiles/static/views/
COPY package.json package-lock.json LICENSE /copyfiles/

# Main image build
FROM node:20-bookworm
COPY --from=copyfiles /copyfiles/ /opt/fbsim-ui/
WORKDIR /opt/fbsim-ui
RUN npm install
CMD ["npm", "run", "prod"]</template></code-sample>
<p>For the fbsim-api, the containerization process was slightly more complicated due to the fact that compiling an API built with the Rocket framework involves dynamic linking.&#160; To overcome this challenge I followed <a href="https://rocket.rs/guide/v0.5/deploying/#containerization" target="_blank">the official Rocket containerization guide</a> which uses GNU <code>objcopy</code> to combine all dynamically linked libraries together into the main API server executable.<br></p>
<h3 data-original-level="H3" id="header-8d51747e-43ad-e4c2-c88b-90c4dc4c279a">GitHub Actions CI<br></h3>
<p>While the above Containerfiles can be built locally, it is more typical to develop a <a href="https://en.wikipedia.org/wiki/Continuous_integration" target="_blank">continuous integration (CI)</a> process that regularly builds the application images when new code is contributed.&#160; <a href="https://github.com/features/actions" target="_blank">GitHub Actions</a> provides this mechanism directly built into the GitHub platform, and I found it to be very reliable and useful for containerized application development on public GitHub.</p>
<p>GitHub Actions workflows are placed in <a href="https://github.com/whatsacomputertho/fbsim-api/tree/main/.github/workflows" target="_blank">the .github/workflows subdirectory</a> of the repository.&#160; The workflows are defined by YAML specifications which defines properties of the workflow such as</p>
<ol><li role="textbox">The ​steps ​of the workflow</li><li role="textbox">The ​script ​executed ​during ​each ​step ​of ​the ​workflow</li><li role="textbox">Which ​events ​trigger ​the ​workflow</li></ol>
<p>and more.&#160; My workflows for the <a href="https://github.com/whatsacomputertho/fbsim-ui/blob/main/.github/workflows/image-build.yaml" target="_blank">fbsim-ui</a> and <a href="https://github.com/whatsacomputertho/fbsim-api/blob/main/.github/workflows/image-build.yaml" target="_blank">fbsim-api</a> are very simple, they first extract the package version from the package manifest; this is later used to tag the image.&#160; They then build the image, and push the image into ghcr.io, GitHub's built-in container image registry.&#160; Below is the full GitHub actions workflow for the fbsim-api at the time of writing this blog.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">name: FBSim API build
on:
  push:
    branches: [ "main" ]
jobs:
  build:
    permissions: write-all
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Build and push the FBSim API image
      run: |
        # Get the package version from cargo.toml
        echo "[INFO] Getting rust package version from cargo.toml"
        RUST_PACKAGE_VERSION=$(
          awk -F ' = ' '$1 ~ /version/ { gsub(/[\"]/, "", $2); printf("%s",$2) }' Cargo.toml
        );
        echo "[DEBU] Package version: ${RUST_PACKAGE_VERSION}"
        
        # Build the FBSim API image
        echo "[INFO] Building and pushing ghcr.io/whatsacomputertho/fbsim-api:v${RUST_PACKAGE_VERSION}"
        docker login \
          -u whatsacomputertho \
          -p ${{ secrets.GITHUB_TOKEN }} \
          ghcr.io/whatsacomputertho
        docker build . \
          -f Containerfile \
          --tag ghcr.io/whatsacomputertho/fbsim-api:v${RUST_PACKAGE_VERSION}
        docker push \
          ghcr.io/whatsacomputertho/fbsim-api:v${RUST_PACKAGE_VERSION}</template></code-sample>
<h3 data-original-level="H3" id="header-1e88bc0c-4aad-4d70-84da-b3aafd70090e">GitHub Container Registry<br></h3>
<p><a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry" target="_blank">The GitHub container image registry</a>, ghcr.io (also sometimes called "<a href="https://docs.github.com/en/packages" target="_blank">GitHub Packages</a>") is used in the above example to publish the images built in my GitHub Actions workflows.&#160; In order to publish the images, a GitHub token with the proper permissions must be used.&#160; Conveniently, GitHub actions automatically generates a short-lived token on each run that can be used to publish my images, as seen above when I reference <code>${{ secrets.GITHUB_TOKEN }}</code>.</p>
<p>The &#8203;GitHub &#8203;container &#8203;image &#8203;registry &#8203;is &#8203;publicly &#8203;readable, &#8203;meaning &#8203;that &#8203;anyone &#8203;can pull the container images built and published by my workflows.&#160; So you, reading this at home, can deploy FBSim using the exact same image references as I use for my deployment - we will both pull the images from ghcr.io.<br></p>
<h2 data-original-level="H2" id="header-2a0e0370-3025-9626-8d5c-143633e3c2cd">Deployment</h2>
<p>Now comes the main task of actually deploying the FBSim application.&#160; I started with the simplest task of deploying FBSim on my local machine using docker compose.&#160; Then later, I worked through the task of deploying FBSim publicly on an AWS VM using docker compose.&#160; Finally, I worked through the same exercise of deploying FBSim publicly, but this time on a single-node K3s cluster also hosted on an AWS VM.<br></p>
<h3 data-original-level="H3" id="header-19182742-e79f-92da-38d3-4255fdad79c0">&#8203;Compose</h3>
<p><a href="https://github.com/whatsacomputertho/fbsim-deploy/blob/main/deploy/compose/local/docker-compose.yaml" target="_blank">The local docker compose deployment</a> was the easiest exercise, it simply involved coordinating the port numbers of the two applications.&#160; The UI is equipped with an environment variable which it uses to determine where to send its API requests, and so the main exercise was setting this environment variable so that it matched the port number behind which the API was exposed on my local host.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">services:
  api:
    image: ghcr.io/whatsacomputertho/fbsim-api:${FBSIM_API_VERSION:-v1.0.0-alpha.1}
    ports:
      - "8080:8080"
  ui:
    image: ghcr.io/whatsacomputertho/fbsim-ui:${FBSIM_UI_VERSION:-v1.0.0-alpha.1}
    ports:
      - "8081:8081"
    environment:
      - FBSIM_UI_DOMAIN=0.0.0.0
      - FBSIM_UI_PORT=8081
      - FBSIM_API_HOST=http://localhost:8080</template></code-sample>
<p>This would then serve as a building block toward <a href="https://github.com/whatsacomputertho/fbsim-deploy/blob/main/deploy/compose/prod/README.md" target="_blank">the "production" docker compose deployment</a>.&#160; That said, the &#8203;"production" docker compose deployment would be a bit trickier.&#160; The main challenge in this case would be <a href="https://en.wikipedia.org/wiki/Public_key_certificate" target="_blank">the acquisition of a TLS certificate which is signed by a valid certification authority</a>.&#160; This is a must-have for secure communication between the UI and API server; without it most browsers would flag my site as "potentially malicious".</p>
<p>To overcome this challenge I used a free set of tools called <a href="https://certbot.eff.org/" target="_blank">Certbot</a> and <a href="https://letsencrypt.org/" target="_blank">Let's Encrypt</a>.  Let's Encrypt is hosted by the <a href="https://www.abetterinternet.org/" target="_blank">Internet Security Research Group (ISRG)</a>, and offers a free and automated solution for acquiring signed TLS certificates signed by the ISRG's CA.  Certbot is a containerized utility which we will issue the certificate signing request against our domain.</p>
<p>Once the certificate signing request is issued, we must have a server running which can successfully respond to an <a href="https://letsencrypt.org/docs/challenge-types/#http-01-challenge" target="_blank">Automatic Certificate Management Environment (ACME)</a> challenge.&#160; Once it does, Certbot will store the granted certificates within a volume on the host.&#160; We will configure a containerized <a href="https://nginx.org/" target="_blank">NGINX web server</a> to respond to the request for us.&#8203;</p>
<p>Here is <a href="https://github.com/whatsacomputertho/fbsim-deploy/blob/main/build/acme-responder/nginx.conf" target="_blank">the NGINX configuration</a> for <a href="https://github.com/whatsacomputertho/fbsim-deploy/tree/main/build/acme-responder" target="_blank">the ACME challenge responder</a>.&#160; You'll notice that my domain is hardcoded in the configuration - this is something that I'd like to read in from an environment variable in the future so that others can more easily reproduce this deployment themselves.<br></p>
<code-sample type="json" copy-clipboard-button><template preserve-content="preserve-content">events {
    worker_connections 1024;
}

http {
    server {
        listen 80;
        server_name whatsacomputertho.com;
        resolver 127.0.0.11;

        location /.well-known/acme-challenge/ {
                root /var/www/certbot;
        }
    }
}</template></code-sample>
<p>This NGINX server will simply respond to requests to the <code>/.well-known/acme-challenge/&lt;TOKEN&gt;</code> path by placing the given token into a file in <code>/var/www/certbot</code>. &#8203; &#8203;We &#8203;will &#8203;run &#8203;the server and the certbot container such that they both mount the same host volume <code>./data/certbot/www</code> into <code>/var/www/certbot</code>.&#160; This means that when the above NGINX server writes its token into <code>/var/www/certbot</code>, it will be written to <code>./data/certbot/www</code> on the host machine, and then Certbot will be able to read it from that same location.</p>
<p>Certbot will then mount an additional host volume <code>./data/certbot/conf</code> into <code>/etc/letsencrypt</code> which is where it will write the signed certificate, and where later we will mount the certificate.&#160; A demonstration of how the certificate signing request is completed against my domain is given below.&#8203;</p>
<code-sample type="javascript" copy-clipboard-button><template preserve-content="preserve-content"># Run the acme-responder container
docker run -d -it -p 80:80 --volume ./data/certbot/www:/var/www/certbot ghcr.io/whatsacomputertho/fbsim-acme-responder:v1.0.0-alpha.1

# Run the certbot container in dry run mode
# Select option 2
#     2: Saves the necessary validation files to a .well-known/acme-challenge/
#        directory within the nominated webroot path. A separate HTTP server must be
#        running and serving files from the webroot path. HTTP challenge only (wildcards
#        not supported). (webroot)
docker run -it --volume ./data/certbot/www:/var/www/certbot --volume ./data/certbot/conf:/etc/letsencrypt certbot/certbot certonly -d whatsacomputertho.com --webroot-path /var/www/certbot --dry-run</template></code-sample>
<p>Once we have the certificates, we can deploy our application.&#160; In order to do so, we will <a href="https://github.com/whatsacomputertho/fbsim-deploy/blob/main/build/proxy/nginx.conf" target="_blank">configure another containerized NGINX web server</a> which will serve as <a href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank">a "reverse proxy"</a>.&#160; All traffic into the application will be pointed at this web server.&#160; The server will mount our TLS certificates and apply TLS encryption, and then simply route all traffic through itself to the UI and API servers.<br></p>
<code-sample type="json" copy-clipboard-button><template preserve-content="preserve-content">events {
    worker_connections 1024;
}

http {
    server {
        listen 443 ssl;
        ssl_certificate /etc/letsencrypt/live/whatsacomputertho.com/fullchain.pem;
        ssl_certificate_key /etc/letsencrypt/live/whatsacomputertho.com/privkey.pem;
        include /etc/letsencrypt/options-ssl-nginx.conf;
        ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;
        server_name whatsacomputertho.com;
        resolver 127.0.0.11;

        location /.well-known/acme-challenge/ {
                root /var/www/certbot;
        }
        location = / {
                set $upstream ui:8081;
                proxy_pass http://$upstream/;
                proxy_set_header X-Real-IP $remote_addr;
        }
        location = /game/sim {
                set $upstream api:8080;
                proxy_pass http://$upstream/game/sim;
                proxy_set_header X-Real-IP $remote_addr;
        }
        location ~ ^/js/(.*)$ {
                set $upstream ui:8081;
                proxy_pass http://$upstream/js/$1;
                proxy_set_header X-Real-IP $remote_addr;
        }
        location ~ ^/img/(.*)$ {
                set $upstream ui:8081;
                proxy_pass http://$upstream/img/$1;
                proxy_set_header X-Real-IP $remote_addr;
        }
        location ~ ^/css/(.*)$ {
                set $upstream ui:8081;
                proxy_pass http://$upstream/css/$1;
                proxy_set_header X-Real-IP $remote_addr;
        }
    }

    server {
        listen 80;
        server_name whatsacomputertho.com;
        return 302 https://$server_name$request_uri;
    }
}</template></code-sample>
<p>Here is another scenario where my domain is hardcoded into the NGINX configuration, but in the future I would like to parameterize it so that others can repeat this docker-compose-based deployment.</p>
<p>As &#8203;you &#8203;may &#8203;have &#8203;guessed, <a href="https://github.com/whatsacomputertho/fbsim-deploy/tree/main/build/proxy" target="_blank">this NGINX reverse proxy is also containerized</a>, and mounts the <code>./data/cetbot/conf</code>&#160;host directory (which is where Certbot wrote our certificates and configuration) into <code>/etc/letsencrypt</code>.&#160; Also you may notice that throughout the NGINX configuration, I use an alias for my UI and API servers.&#160; This is possible because, when spinning up a docker-compose deployment, <a href="https://docs.docker.com/compose/how-tos/networking/" target="_blank">docker-compose sets up a virtual network</a> mapping our service names to their local IPs.</p>
<p>Here &#8203;is &#8203;my &#8203;final &#8203;<code>docker-compose.yaml</code>:</p>
<code-sample type="javascript" copy-clipboard-button><template preserve-content="preserve-content">services:
  proxy:
    image: ghcr.io/whatsacomputertho/fbsim-proxy:${FBSIM_PROXY_VERSION:-v1.0.0-alpha.1}
    ports:
      - 443:443
      - 80:80
    volumes:
      - ./data/certbot/conf:/etc/letsencrypt
  api:
    image: ghcr.io/whatsacomputertho/fbsim-api:${FBSIM_API_VERSION:-v1.0.0-alpha.1}
    ports:
      - "8080:8080"
  ui:
    image: ghcr.io/whatsacomputertho/fbsim-ui:${FBSIM_UI_VERSION:-v1.0.0-alpha.1}
    ports:
      - "8081:8081"
    environment:
      - FBSIM_UI_DOMAIN=0.0.0.0
      - FBSIM_UI_PORT=8081
      - FBSIM_API_HOST=${FBSIM_HOST:-https://whatsacomputertho.com}</template></code-sample>
<p>All &#8203;that is needed to spin up our services at this point is simply run a <code>docker compose up -d</code>from the directory in which this <code>docker-compose.yaml</code> is located, and our services will spin up with TLS encryption enabled.<br></p>
<h3 data-original-level="H3" id="header-0dcd74b5-ca39-679e-33f0-e8703788b862">Kubernetes<br></h3>
<p>Something I've noticed is, a lot of people seem to think the benefits of containerization end at docker and docker-compose, when really these are just the basic building blocks for something much larger.&#160; <a href="https://kubernetes.io/" target="_blank">Kubernetes</a> is a <a href="https://en.wikipedia.org/wiki/Computer_cluster" target="_blank">cluster computing</a> and <a href="https://www.redhat.com/en/topics/containers/what-is-container-orchestration" target="_blank">container orchestration</a> platform which automates many aspects of deploying and managing containerized and "cloud native" software.&#160; I work with Kubernetes heavily in my role at IBM, and so I felt inclined for this project to deploy my FBSim application on Kubernetes.</p>
<p>In &#8203;particular, &#8203;I &#8203;have the most experience with <a href="https://k3s.io/" target="_blank">a distribution of Kubernetes called K3s</a>.&#160; K3s is a lightweight Kubernetes distribution which is designed for a quick startup with minimal required configuration.&#160; It can be installed on a single VM and extended across a cluster of VMs, making it easy to pick up and scale as your infrastructure requirements grow. &#8203; &#8203;To &#8203;install &#8203;K3s &#8203;on &#8203;a &#8203;VM &#8203;is &#8203;as &#8203;simple as piping their install script into bash, giving you a Kubernetes installation in seconds.<br></p>
<code-sample type="html" copy-clipboard-button><template preserve-content="preserve-content">curl -sfL https://get.k3s.io | sh -</template></code-sample>
<p>For this exercise, I simply installed single-node K3s, meaning I ran K3s on a single VM and did not extend it across a cluster of VMs.</p>
<p>Kubernetes is a declarative platform.&#160; It manages different types of resources, which the user defines in a manifest file telling Kubernetes what sort of properties that resource should have.&#160; Kubernetes then works to reconcile the resource into that state, comparable to a state machine.&#160; It further supports extensions which define and manage custom resources, around which there is a large ecosystem.</p>
<p>We &#8203;will &#8203;use &#8203;a &#8203;combination &#8203;of &#8203;native &#8203;Kubernetes resources and custom resources to deploy our application publicly with signed TLS certificates.&#160; We will start with the simplest resource type, the Namespace.&#160; Namespaces organize Kubernetes resources into sort of self-contained workspaces; they are commonly used when multiple teams work on the same Kubernetes cluster - each team gets their own Namespace to work with.&#160; I created a Namespace called fbsim for my application.<br></p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: v1
kind: Namespace
metadata:
  name: "fbsim"
  labels:
    name: "fbsim"</template></code-sample>
<p>Next &#8203;up &#8203;is &#8203;our &#8203;Deployment &#8203;resource. &#8203; &#8203;A &#8203;Kubernetes &#8203;Deployment &#8203;is generally used for stateless services, that is, services which do not store and track data over time.&#160; Ours simply respond to HTTP requests and that is all.&#160; A Deployment will manage a resource called a ReplicaSet, which itself will manage resources called Pods.</p>
<p>Pods are comparable to our docker compose file in that they define the containers to deploy, the ports behind which to expose them, the environment variables and volumes they mount, and more.&#160; ReplicaSets then create multiple duplicates of Pods and load balance traffic coming into the Pods.</p>
<p>Deployments finally tie this all together by allowing us to scale up and down the size of our ReplicaSet.&#160; Maybe one day we want 3 replicas of our service, but then another day we want 2.&#160; It is also worth noting that Deployments can be amended with what is called an AutoScaler, which can automatically scale up and down the deployed replicas based on things like CPU usage, or network traffic.&#8203; &#8203; &#8203;In my case, I kept it as simple as possible and simply deployed one replica of my FBSim Pod.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: apps/v1
kind: Deployment
metadata:
  name: "fbsim"
  namespace: "fbsim"
  labels:
    app.kubernetes.io/name: "fbsim"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: "fbsim"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: "fbsim"
    spec:
      containers:
        - name: fbsim-ui
          image: "ghcr.io/whatsacomputertho/fbsim-ui:v1.0.0-alpha.1"
          env:
            - name: FBSIM_UI_DOMAIN
              value: "0.0.0.0"
            - name: FBSIM_UI_PORT
              value: "8081"
            - name: FBSIM_API_HOST
              value: "https://api.fbsim.whatsacomputertho.com"
          ports:
            - containerPort: 8081
              name: fbsim-ui
        - name: fbsim-api
          image: "ghcr.io/whatsacomputertho/fbsim-api:v1.0.0-alpha.1"
          ports:
            - containerPort: 8080
              name: fbsim-api</template></code-sample>
<p>I &#8203;then &#8203;mapped &#8203;a &#8203;Service resource to my Deployment, which exposes the Deployment to the cluster's network.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: v1
kind: Service
metadata:
  name: "fbsim"
  namespace: "fbsim"
  labels:
    app.kubernetes.io/name: "fbsim"
spec:
  selector:
    app.kubernetes.io/name: "fbsim"
  ports:
    - name: fbsim-ui
      protocol: TCP
      port: 8081
      targetPort: 8081
    - name: fbsim-api
      protocol: TCP
      port: 8080
      targetPort: 8080</template></code-sample>
<p>At this point my Deployment still is not publicly accessible.&#160; We will need to use our first custom resource, called an Ingress, to make our Deployment publicly accessible.&#160; The Ingress-Nginx controller and its custom resource definitions come pre-installed on K3s, so we do not need to do anything special to create these custom resources on our K3s cluster.</p>
<p>I created two Ingresses, one for the UI and another for the API.&#160; Ultimately, these ingresses do roughly the same thing as our reverse proxy did in our docker compose deployment, plus some additional networking tasks, such as creating DNS entries for our services.&#8203; &#8203; &#8203;Below I'll include the UI ingress as an example.<br></p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "fbsim-ui-tls-ingress"
  namespace: "fbsim"
  labels:
    app.kubernetes.io/name: "fbsim-ui-tls-ingress"
  annotations:
    spec.ingressClassName: traefik
    traefik.ingress.kubernetes.io/router.middlewares: "fbsim-fbsim-redirect-https@kubernetescrd"
spec:
  rules:
    - host: "fbsim.whatsacomputertho.com"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "fbsim"
                port:
                  number: 8081</template></code-sample>
<p>You might notice that there is an annotation in the above Ingress referring to traefik router middleware.&#160; This refers to another custom resource I created, Middleware, which is another pre-installed custom resource definition offered by traefik.&#160; My traefik Middleware simply redirects HTTP traffic to HTTPS, much like what I had configured in my NGINX reverse proxy in the docker compose deployment.</p>
<p>But &#8203;wait, if we're redirecting HTTP traffic to HTTPS, then &#8203;how &#8203;do we enable HTTPS / TLS on our Ingress?&#160; We hadn't done anything special to enable that yet.&#160; So in comes Cert Manager - which is another open source extension of Kubernetes (this time it does not come pre-installed on K3s) which basically automates that whole certificate signing dance from our docker compose deployment.</p>
<p>We &#8203;will &#8203;first &#8203;need &#8203;to &#8203;install &#8203;the &#8203;Cert &#8203;Manager custom resource definitions (CRDs) since they aren't pre-installed on K3s.</p>
<code-sample type="html" copy-clipboard-button><template preserve-content="preserve-content">kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/1.16.3/cert-manager.crds.yaml</template></code-sample>
<p>Then &#8203;we &#8203;can &#8203;create an Issuer custom resource.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: "fbsim-letsencrypt-prod"
  namespace: "fbsim"
  labels:
    app.kubernetes.io/name: "fbsim-letsencrypt-prod"
spec:
  acme:
    email: "my@email.com"
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: "fbsim-letsencrypt-prod"
    solvers:
    - http01:
        ingress:
          class: traefik</template></code-sample>
<p>Then &#8203;finally, &#8203;to &#8203;equip &#8203;our &#8203;Ingress &#8203;with &#8203;a &#8203;signed &#8203;certificate from Cert Manager, we just need to update it like so.&#160; Here we add an annotation to our Ingress pointing at our Issuer, which will initiate a certificate signing request in Cert Manager.&#160; Then we also add a TLS configuration section which defines the Kubernetes Secret where the certificate will be stored.</p>
<code-sample type="yaml" copy-clipboard-button><template preserve-content="preserve-content">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "fbsim-ui-tls-ingress"
  namespace: "fbsim"
  labels:
    app.kubernetes.io/name: "fbsim-ui-tls-ingress"
  annotations:
    spec.ingressClassName: traefik
    cert-manager.io/issuer: "fbsim-letsencrypt-prod"
    traefik.ingress.kubernetes.io/router.middlewares: "fbsim-fbsim-redirect-https@kubernetescrd"
spec:
  rules:
    - host: "fbsim.whatsacomputertho.com"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "fbsim"
                port:
                  number: 8081
  tls:
    - secretName: "fbsim-ui-tls"
      hosts:
        - "fbsim.whatsacomputertho.com"</template></code-sample>
<p>&#8203;I &#8203;haven't &#8203;quite &#8203;explained &#8203;how &#8203;we &#8203;go &#8203;about &#8203;creating all these resources on our K3s cluster.&#160; We can, from one of our VMs running K3s, run <code>kubectl create -f [manifest-file]</code> to freshly create each of our resources from their respective manifest files, or <code>kubectl apply -f [manifest-file]</code> if we want to update any of our resources.&#160; That said, in my case, I created something called a Helm chart, which allows us to template our resources in different ways and install our entire application at once as a package.</p>
<p>You &#8203;can &#8203;find <a href="https://github.com/whatsacomputertho/fbsim-deploy/tree/main/deploy/k8s" target="_blank">​my ​helm ​chart here</a>, and <a href="https://github.com/whatsacomputertho/fbsim-deploy/blob/main/deploy/k8s/README.md" target="_blank">full instructions for deploying the helm chart here</a>.<br></p>
