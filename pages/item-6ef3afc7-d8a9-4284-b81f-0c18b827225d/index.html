
<p>The first step of producing software capable of randomly generating a real-world phenomenon (such as football scores) is simple: get the real-world data.&#160; In this blog, I will reflect on the web scraper I developed to acquire data which was ultimately used to produce my FootballSim box score generator model.</p>
<h2 data-original-level="H2" id="header-105e9674-4d11-0032-773b-c39f10a6e30c">Data &#8203;Retrieval</h2>
<p>The &#8203;first &#8203;step &#8203;of statistical modeling against real-world data is, well, having real-world data.&#160; So I would need to start my journey by identifying a data source, and writing some automation to retrieve and structure its data for me.</p>
<p>In &#8203;my &#8203;case, &#8203;I'm looking for football box scores, and to my delight I quickly discovered <i target="_blank"><a href="https://www.footballdb.com/" target="_blank">The Football Database</a></i> which is a near perfect data source for me.&#160; It contains <a href="https://www.footballdb.com/games/index.html" role="textbox">the entire super bowl era of box scores</a>, and even box scores dating well before the super bowl era.&#160; The only real challenge I would need to overcome in leveraging The Football Database is the fact that it does not expose any sort of REST API for retrieving its data.&#160; I would instead need to write a web scraper for scraping its box score data from its HTML.</p>
<h3 data-original-level="H3" id="header-4903677b-584b-8eb1-8705-93a884fb47c4">What's Web Scraping, Though?<br></h3>
<p>Web &#8203;scraping is a form of automated data retrieval which involves parsing web content (typically in HTML format) and reformatting it into a more structured format for storage and analysis.&#160; In large scale data retrieval systems, web scraping will take place after web crawling.&#160; A web crawler will traverse websites for content and store the raw, unprocessed content, which will later be processed and stored by a web scraper.</p>
<p>OpenAI's large language models, such as GPT-3 and 4 powering their ChatGPT software, were trained on content gathered and later scraped using web crawlers and scrapers.&#160; Since then, significant effort has been made to limit web crawling and web scraping across various websites.&#160; As such, your experience scraping web content may not be as forgiving as mine was.<br></p>
<h3 data-original-level="H3" id="header-493161e3-7da1-b8c8-3bd7-30a81327f228">The &#8203;Football &#8203;Database &#8203;Web &#8203;Scraper</h3>
<p>I developed <a href="https://github.com/whatsacomputertho/fbdb-webscrape" target="_blank">a Rust-based CLI</a> for scraping box score data from The Football Database's HTML, and storing the data in JSON files locally.</p>
<p>For this iteration, I only really needed to implement one command - <code>fbdb-webscrape boxscores</code>.</p>
<code-sample type="json" copy-clipboard-button><template preserve-content="preserve-content">Retrieve historic box scores from the football database

Usage: fbdb-webscrape[.exe] boxscores [OPTIONS]

Options:
  -o, --output [output_format]  The format to output
  -f, --file [output_file]      The file to write to
  -y, --year [year]             The year of scores to retrieve [default: 2024]
  -h, --help                    Print help</template></code-sample>
<p>This command allows the user to input a year, an output format, and an output file, and it will parse that year of box scores from The Football Database.&#160; In the future, I plan to circle back to this CLI and add new commands which can scrape the data I may need for my next iterations of this project.</p>
<p>At the time of writing this blog, the &#8203;project &#8203;consists &#8203;of &#8203;three Rust source files:</p>
<ul><li role="textbox"><code target="_blank">main.rs</code>: Contains the main function which runs on execution of the CLI.  Also contains some helper functions for requesting The Football Database's HTML using <a href="https://docs.rs/reqwest/latest/reqwest/" target="_blank">reqwest</a> and scraping it using <a href="https://docs.rs/scraper/latest/scraper/" target="_blank">scraper</a> into a <code>BoxScore</code> instance.<br></li><li role="textbox"> <code target="_blank">cli.rs</code>: Contains the CLI definition.  For this project I used <a href="https://docs.rs/clap/latest/clap/_derive/index.html" target="_blank">clap_derive</a>, which is an extension of the <a href="https://docs.rs/clap/latest/clap/index.html" target="_blank">clap</a> crate in which the CLI is structured as a collection of structs and enums equipped with the proper trait implementations using derive macros.<br></li><li role="textbox"><code target="_blank">boxscore.rs</code>: Contains the <code>BoxScore</code> struct implementation.  I used <a href="https://serde.rs/" target="_blank">serde</a> and <a href="https://docs.rs/serde_json/latest/serde_json/" target="_blank">serde_json</a> to enable the <code>BoxScore</code> struct to be easily serialized into JSON format and deserialized from JSON format.</li></ul>
<p>From a web scraping perspective <a href="https://github.com/whatsacomputertho/fbdb-webscrape/blob/179ab48d23d8d38afe250ddd20988b51747b2c13/src/main.rs#L1-L108" target="_blank">the helper functions</a> which make use of reqwest and scraper are really the stars of the show.&#160; The fundamental aspects of these functions are given in code samples below.&#160; Firstly, I instantiated a reqwest client and used it to fetch a webpage:<br></p>
<code-sample type="javascript" copy-clipboard-button><template preserve-content="preserve-content">/// Get the box scores HTML response from the football database as a string
fn get_html_string() -&gt; String {
    // Initialize a reqwest blocking client
    let client = reqwest::blocking::Client::new();
    
    // Query the webserver and get a Response
    // The response is wrapped in a Future and a Result
    let res = client.get("https://example.com/")
        .header(USER_AGENT, "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36")
        .send();
    
    // Get the response text as a String and return it
    let res_str = res.unwrap().text().unwrap();
    return res_str;
}</template></code-sample>
<p>Then, I instantiated an HTML scraper and used it to parse the HTML.&#160; One particularly useful way to parse the HTML is to use CSS selectors (which are provided by scraper via its <code>Selector</code> struct), shown below.<br></p>
<code-sample type="javascript" copy-clipboard-button><template preserve-content="preserve-content">/// Parse all paragraph elements from example.com
fn parse_box_scores(res_html: &amp;String) -&gt; Vec&lt;String&gt; {
    // Initialize a mutable vector of strings to append to
    let mut paragraph_strings: Vec&lt;String&gt; = Vec::new();

    // Initialize an HTML scraper from the response HTML
    let document = Html::parse_document(res_html);

    // Select all paragraph elements
    let paragraph_selector = Selector::parse("p").unwrap();
    let paragraphs = document.select(&amp;paragraph_selector);

    // For each paragraph element, get its text and append
    for paragraph in paragraphs {
        let paragraph_string: String = paragraph.text();
        paragraph_strings.push(paragraph_string);
    }

    // Return the vector of paragraph strings
    paragraph_strings
}</template></code-sample>
<p></p>
<p>I could have implemented these helper functions a bit more properly.&#160; Looking forward, I may refactor the&#160;functions for fetching the HTML into a client struct, and then refactor the parsing functions into a "from" trait on my <code>BoxScore</code> struct.</p>
<p>This being my first web scraping project, one of the main hurdles I faced was working around 403 error codes when fetching the HTML from The Football Database's web server.&#160; It turns out, web servers will typically block requests for HTML content unless the request header identifies the agent being used to load the content.&#160; In my case, I needed to add <a href="https://github.com/whatsacomputertho/fbdb-webscrape/blob/179ab48d23d8d38afe250ddd20988b51747b2c13/src/main.rs#L25" target="_blank">a User-Agent header mimicking a web browser</a> before it would return any HTML content.</p>
<p>Beyond &#8203;the &#8203;web &#8203;scraping &#8203;aspect of the project, I also spent a lot of time fiddling with clap_derive.&#160; I find clap_derive to be very useful for standing up a quick CLI, but very difficult to fine tune over time.&#160; This is partially because documentation is limited and discussion online typically refers to the core clap module.&#160; The types instantiated when using clap_derive differ from those instantiated using clap, leading to occasional confusing "method not found" errors.</p>
<p>Despite a few challenges along the way, I was able to finish this first implementation in about a day's worth of programming, which I consider a success.&#160; More information on its usage can be found <a href="https://github.com/whatsacomputertho/fbdb-webscrape/blob/main/README.md" target="_blank">in its readme file</a>.<br></p>
<h2 data-original-level="H2" id="header-b0215f3b-2189-a188-59af-604d9f5936d7">Next &#8203;Steps</h2>
<p>Now that we have our box score data, we will need to better understand it.&#160; Our next step will be to conduct what is called <b>Exploratory Data Analysis</b>, which involves summarizing and visualizing the data in various ways to attempt to identify trends which can be modeled.&#160; This will be discussed in detail in the next blog post in this series, <i>FootballSim Box Score Generator Model</i>.<b></b><br></p>
